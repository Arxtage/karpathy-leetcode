{
  "id": "07-gpt-ex019",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg13",
  "title": "Add Residual Connections",
  "difficulty": "medium",
  "order": 19,
  "runtime": "local",
  "topics": [
    "residual-connections",
    "skip-connections"
  ],
  "description": "Implement a `Block` module that uses residual (skip) connections around the self-attention and feed-forward sub-layers.\n\nYour class should:\n- Accept `n_embd`, `n_head`, and `block_size` as constructor arguments\n- Create a `MultiHeadAttention` module (`self.sa`) and a `FeedForward` module (`self.ffwd`)\n- In `forward(self, x)`, apply each sub-layer with a residual connection: add the sub-layer's output back to its input\n\nResidual connections help gradients flow through deep networks by providing a shortcut path.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Your Implementation ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8):\n        super().__init__()\n        head_size = n_embd // n_head\n        # TODO: Create self.sa (MultiHeadAttention) and self.ffwd (FeedForward)\n        pass\n\n    def forward(self, x):\n        # TODO: Apply self-attention with residual connection\n        # TODO: Apply feed-forward with residual connection\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Solution ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n        self.ffwd = FeedForward(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(x)\n        x = x + self.ffwd(x)\n        return x\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nB, T, C = 2, 8, 32\nn_head = 4\nx = torch.randn(B, T, C)\nblock = Block(C, n_head, block_size=T)\n\ntry:\n    out = block(x)\n    assert out.shape == (B, T, C), f\"Expected shape {(B, T, C)}, got {out.shape}\"\n    print(f\"PASS: output shape matches input shape {out.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: output shape matches input shape — {e}\")\n\ntry:\n    assert hasattr(block, 'sa'), \"Block should have 'sa' attribute\"\n    assert hasattr(block, 'ffwd'), \"Block should have 'ffwd' attribute\"\n    print(\"PASS: block has sa and ffwd attributes\")\nexcept Exception as e:\n    print(f\"FAIL: block has sa and ffwd attributes — {e}\")\n\ntry:\n    x_test = torch.randn(B, T, C, requires_grad=True)\n    out = block(x_test)\n    loss = out.sum()\n    loss.backward()\n    assert x_test.grad is not None, \"Gradient should flow to input\"\n    assert x_test.grad.abs().sum() > 0, \"Gradient should be non-zero\"\n    print(\"PASS: gradient flows through residual connections\")\nexcept Exception as e:\n    print(f\"FAIL: gradient flows through residual connections — {e}\")\n\ntry:\n    # Check residual: output should not be identical to sublayer output alone\n    x_test = torch.randn(B, T, C)\n    out = block(x_test)\n    sa_only = block.sa(x_test)\n    # With residual, out should differ from just sa output\n    assert not torch.allclose(out, sa_only, atol=1e-5), \"Output should include residual (x + sa(x)), not just sa(x)\"\n    print(\"PASS: residual connections are applied (output != sublayer output alone)\")\nexcept Exception as e:\n    print(f\"FAIL: residual connections are applied — {e}\")\n",
  "hints": [
    "A residual connection means: x = x + sublayer(x). The input is added back to the output of the sublayer.",
    "Apply the pattern twice: once for self-attention (x = x + self.sa(x)) and once for feed-forward (x = x + self.ffwd(x)).",
    "head_size = n_embd // n_head ensures that when all heads are concatenated, the total dimension equals n_embd."
  ]
}