{
  "id": "07-gpt-ex008",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg06",
  "title": "Generate Text from a Model",
  "difficulty": "easy",
  "order": 8,
  "topics": ["text-generation", "sampling"],
  "runtime": "local",
  "description": "Implement a `generate(model, idx, max_new_tokens)` function that autoregressively generates new tokens from a language model.\n\nThe function should:\n- Start with the given `idx` tensor of shape `(B, T)` (a batch of existing token sequences).\n- For each new token to generate:\n  - Get logits from the model for the current sequence.\n  - Focus only on the **last time step** of the logits.\n  - Apply softmax to get probabilities.\n  - Sample the next token from the probability distribution.\n  - Append it to the running sequence.\n- Return the extended sequence tensor of shape `(B, T + max_new_tokens)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndef decode(l):\n    return ''.join([itos[i] for i in l])\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            logits = logits.view(B, T, -1)\n        return logits, loss\n\nmodel = BigramLanguageModel(vocab_size)\n\ndef generate(model, idx, max_new_tokens):\n    \"\"\"\n    Generate max_new_tokens new tokens, appending to idx.\n\n    Args:\n        model: a language model with a forward(idx) method returning (logits, loss)\n        idx: (B, T) tensor of current token indices\n        max_new_tokens: number of new tokens to generate\n\n    Returns:\n        (B, T + max_new_tokens) tensor of token indices\n    \"\"\"\n    # TODO: loop max_new_tokens times:\n    #   1. get logits from model\n    #   2. focus on the last time step\n    #   3. apply softmax to get probabilities\n    #   4. sample from the distribution\n    #   5. append sampled token to the sequence\n    pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndef decode(l):\n    return ''.join([itos[i] for i in l])\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            logits = logits.view(B, T, -1)\n        return logits, loss\n\nmodel = BigramLanguageModel(vocab_size)\n\ndef generate(model, idx, max_new_tokens):\n    \"\"\"\n    Generate max_new_tokens new tokens, appending to idx.\n\n    Args:\n        model: a language model with a forward(idx) method returning (logits, loss)\n        idx: (B, T) tensor of current token indices\n        max_new_tokens: number of new tokens to generate\n\n    Returns:\n        (B, T + max_new_tokens) tensor of token indices\n    \"\"\"\n    for _ in range(max_new_tokens):\n        logits, _ = model(idx)\n        logits = logits[:, -1, :]  # (B, C)\n        probs = F.softmax(logits, dim=-1)  # (B, C)\n        idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n        idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n    return idx\n",
  "testCode": "try:\n    start = torch.zeros((1, 1), dtype=torch.long)\n    result = generate(model, start, max_new_tokens=10)\n    assert result.shape == (1, 11), f\"Expected shape (1, 11), got {result.shape}\"\n    print(\"PASS: output shape is (1, T + max_new_tokens)\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is (1, T + max_new_tokens) — {e}\")\n\ntry:\n    start = torch.zeros((1, 1), dtype=torch.long)\n    result = generate(model, start, max_new_tokens=20)\n    assert result.shape[1] == 21, f\"Expected 21 tokens, got {result.shape[1]}\"\n    print(\"PASS: output has correct number of tokens\")\nexcept Exception as e:\n    print(f\"FAIL: output has correct number of tokens — {e}\")\n\ntry:\n    start = torch.zeros((2, 3), dtype=torch.long)\n    result = generate(model, start, max_new_tokens=5)\n    assert result.shape == (2, 8), f\"Expected shape (2, 8), got {result.shape}\"\n    print(\"PASS: works with batch_size > 1\")\nexcept Exception as e:\n    print(f\"FAIL: works with batch_size > 1 — {e}\")\n\ntry:\n    start = torch.zeros((1, 1), dtype=torch.long)\n    result = generate(model, start, max_new_tokens=10)\n    assert (result >= 0).all() and (result < vocab_size).all(), \"All token indices should be in [0, vocab_size)\"\n    print(\"PASS: all generated token indices are valid\")\nexcept Exception as e:\n    print(f\"FAIL: all generated token indices are valid — {e}\")\n\ntry:\n    start = torch.zeros((1, 1), dtype=torch.long)\n    result = generate(model, start, max_new_tokens=10)\n    assert torch.equal(result[:, :1], start), \"Original tokens should be preserved at the start\"\n    print(\"PASS: original input tokens are preserved\")\nexcept Exception as e:\n    print(f\"FAIL: original input tokens are preserved — {e}\")\n",
  "hints": [
    "In the loop, get logits with: logits, _ = model(idx). Then focus on the last time step: logits = logits[:, -1, :].",
    "Convert logits to probabilities with F.softmax(logits, dim=-1), then sample with torch.multinomial(probs, num_samples=1).",
    "Append the new token with torch.cat((idx, idx_next), dim=1) and return idx after the loop."
  ]
}