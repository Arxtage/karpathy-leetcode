{
  "id": "07-gpt-ex016",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg12",
  "title": "Multi-Head Attention",
  "difficulty": "hard",
  "order": 16,
  "runtime": "local",
  "topics": [
    "multi-head-attention",
    "nn-module"
  ],
  "description": "Implement a `MultiHeadAttention` module that runs multiple attention heads in parallel and concatenates their outputs.\n\nYour class should:\n- Accept `num_heads` and `head_size` as constructor arguments\n- Create a list of `Head` modules (one per head) using `nn.ModuleList`\n- In `forward(self, x)`, run each head on the input and concatenate the results along the last (channel) dimension\n\nThe `Head` class is provided as prerequisite code. Your output tensor should have shape `(B, T, num_heads * head_size)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Your Implementation ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        # TODO: Create a ModuleList of Head modules\n        pass\n\n    def forward(self, x):\n        # TODO: Run all heads in parallel and concatenate outputs\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Solution ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nB, T, C = 2, 8, 32\nnum_heads = 4\nhead_size = 8\nx = torch.randn(B, T, C)\nmha = MultiHeadAttention(num_heads, head_size, n_embd=C, block_size=T)\n\ntry:\n    out = mha(x)\n    assert out.shape == (B, T, num_heads * head_size), f\"Expected shape {(B, T, num_heads * head_size)}, got {out.shape}\"\n    print(f\"PASS: output shape is correct {out.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is correct — {e}\")\n\ntry:\n    assert isinstance(mha, nn.Module), \"MultiHeadAttention should be an nn.Module\"\n    print(\"PASS: MultiHeadAttention is an nn.Module\")\nexcept Exception as e:\n    print(f\"FAIL: MultiHeadAttention is an nn.Module — {e}\")\n\ntry:\n    assert hasattr(mha, 'heads'), \"Should have a 'heads' attribute\"\n    assert isinstance(mha.heads, nn.ModuleList), \"'heads' should be an nn.ModuleList\"\n    assert len(mha.heads) == num_heads, f\"Expected {num_heads} heads, got {len(mha.heads)}\"\n    print(f\"PASS: has {num_heads} heads in a ModuleList\")\nexcept Exception as e:\n    print(f\"FAIL: has {num_heads} heads in a ModuleList — {e}\")\n\ntry:\n    out = mha(x)\n    assert out.dtype == torch.float32, f\"Expected float32 output, got {out.dtype}\"\n    assert not torch.isnan(out).any(), \"Output contains NaN values\"\n    print(\"PASS: output is valid (no NaN, correct dtype)\")\nexcept Exception as e:\n    print(f\"FAIL: output is valid (no NaN, correct dtype) — {e}\")\n",
  "hints": [
    "Use nn.ModuleList (not a plain Python list) so PyTorch can track the parameters of each head.",
    "In forward, run each head on x and collect the results in a list, then use torch.cat(..., dim=-1) to concatenate along the channel dimension.",
    "The output dimension will be num_heads * head_size because each head outputs (B, T, head_size) and you concatenate num_heads of them."
  ]
}