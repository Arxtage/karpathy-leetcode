{
  "id": "07-gpt-ex020",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg14",
  "title": "Layer Normalization",
  "difficulty": "easy",
  "order": 20,
  "runtime": "local",
  "topics": [
    "layer-norm",
    "normalization"
  ],
  "description": "Modify the `Block` module to add Layer Normalization using the pre-norm formulation.\n\nIn the pre-norm pattern, you normalize the input *before* passing it to each sub-layer (rather than after). Your class should:\n- Add two `nn.LayerNorm` instances: `self.ln1` and `self.ln2`\n- Apply `ln1` before the self-attention sub-layer\n- Apply `ln2` before the feed-forward sub-layer\n- Keep the residual connections from the previous exercise\n\nThe output shape should remain `(B, T, n_embd)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Your Implementation ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n        self.ffwd = FeedForward(n_embd)\n        # TODO: Add two LayerNorm layers: self.ln1 and self.ln2\n        pass\n\n    def forward(self, x):\n        # TODO: Apply pre-norm formulation with residual connections\n        # x = x + self.sa(self.ln1(x))\n        # x = x + self.ffwd(self.ln2(x))\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Solution ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nB, T, C = 2, 8, 32\nn_head = 4\nx = torch.randn(B, T, C)\nblock = Block(C, n_head, block_size=T)\n\ntry:\n    assert hasattr(block, 'ln1'), \"Block should have 'ln1' attribute\"\n    assert hasattr(block, 'ln2'), \"Block should have 'ln2' attribute\"\n    assert isinstance(block.ln1, nn.LayerNorm), \"ln1 should be nn.LayerNorm\"\n    assert isinstance(block.ln2, nn.LayerNorm), \"ln2 should be nn.LayerNorm\"\n    print(\"PASS: block has ln1 and ln2 LayerNorm layers\")\nexcept Exception as e:\n    print(f\"FAIL: block has ln1 and ln2 LayerNorm layers — {e}\")\n\ntry:\n    out = block(x)\n    assert out.shape == (B, T, C), f\"Expected shape {(B, T, C)}, got {out.shape}\"\n    print(f\"PASS: output shape is correct {out.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is correct — {e}\")\n\ntry:\n    assert list(block.ln1.normalized_shape) == [C], f\"ln1 should normalize over {C} dimensions\"\n    assert list(block.ln2.normalized_shape) == [C], f\"ln2 should normalize over {C} dimensions\"\n    print(f\"PASS: LayerNorm normalized_shape is [{C}]\")\nexcept Exception as e:\n    print(f\"FAIL: LayerNorm normalized_shape is [{C}] — {e}\")\n\ntry:\n    x_test = torch.randn(B, T, C, requires_grad=True)\n    out = block(x_test)\n    loss = out.sum()\n    loss.backward()\n    assert x_test.grad is not None and x_test.grad.abs().sum() > 0\n    print(\"PASS: gradient flows through the block\")\nexcept Exception as e:\n    print(f\"FAIL: gradient flows through the block — {e}\")\n",
  "hints": [
    "Pre-norm means you normalize BEFORE the sub-layer, not after: x = x + sublayer(norm(x)).",
    "nn.LayerNorm(n_embd) creates a layer norm that normalizes over the last dimension of size n_embd.",
    "The pattern is: x = x + self.sa(self.ln1(x)) then x = x + self.ffwd(self.ln2(x))."
  ]
}