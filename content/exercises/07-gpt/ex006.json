{
  "id": "07-gpt-ex006",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg05",
  "title": "Embedding Table Lookup",
  "difficulty": "easy",
  "order": 6,
  "topics": ["embeddings", "nn-embedding"],
  "runtime": "local",
  "description": "Create an embedding table using `nn.Embedding` and use it to look up embeddings for a batch of token indices.\n\nGiven a batch of token indices with shape `(B, T)` (batch size by sequence length), passing them through an `nn.Embedding(vocab_size, vocab_size)` layer should produce an output tensor with shape `(B, T, vocab_size)`.\n\nYour task:\n1. Create the embedding table with the right dimensions.\n2. Pass the input `idx` through it to get the output `logits`.",
  "starterCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\n# Create a sample batch: 4 sequences of length 8\nix = torch.randint(len(data) - 8, (4,))\nidx = torch.stack([data[i:i+8] for i in ix])  # shape: (4, 8)\n\n# TODO: create an embedding table using nn.Embedding with dimensions (vocab_size, vocab_size)\nembedding_table = None\n\n# TODO: pass idx through the embedding table to get logits\nlogits = None\n",
  "solutionCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\n# Create a sample batch: 4 sequences of length 8\nix = torch.randint(len(data) - 8, (4,))\nidx = torch.stack([data[i:i+8] for i in ix])  # shape: (4, 8)\n\nembedding_table = nn.Embedding(vocab_size, vocab_size)\n\nlogits = embedding_table(idx)\n",
  "testCode": "try:\n    assert isinstance(embedding_table, nn.Embedding), f\"Expected nn.Embedding, got {type(embedding_table)}\"\n    print(\"PASS: embedding_table is an nn.Embedding\")\nexcept Exception as e:\n    print(f\"FAIL: embedding_table is an nn.Embedding — {e}\")\n\ntry:\n    assert embedding_table.num_embeddings == vocab_size, f\"Expected num_embeddings={vocab_size}, got {embedding_table.num_embeddings}\"\n    assert embedding_table.embedding_dim == vocab_size, f\"Expected embedding_dim={vocab_size}, got {embedding_table.embedding_dim}\"\n    print(\"PASS: embedding_table has correct dimensions\")\nexcept Exception as e:\n    print(f\"FAIL: embedding_table has correct dimensions — {e}\")\n\ntry:\n    assert isinstance(logits, torch.Tensor), f\"logits should be a tensor, got {type(logits)}\"\n    print(\"PASS: logits is a tensor\")\nexcept Exception as e:\n    print(f\"FAIL: logits is a tensor — {e}\")\n\ntry:\n    B, T = idx.shape\n    assert logits.shape == (B, T, vocab_size), f\"Expected logits shape ({B}, {T}, {vocab_size}), got {logits.shape}\"\n    print(\"PASS: logits has shape (B, T, vocab_size)\")\nexcept Exception as e:\n    print(f\"FAIL: logits has shape (B, T, vocab_size) — {e}\")\n",
  "hints": [
    "nn.Embedding(num_embeddings, embedding_dim) creates a lookup table. Here both dimensions are vocab_size.",
    "To look up embeddings, simply call the embedding table like a function: embedding_table(idx).",
    "The output shape will be (*input_shape, embedding_dim), so (B, T) input becomes (B, T, vocab_size)."
  ]
}