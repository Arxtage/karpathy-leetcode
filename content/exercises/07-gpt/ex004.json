{
  "id": "07-gpt-ex004",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg04",
  "title": "Create Training Batches",
  "difficulty": "easy",
  "order": 4,
  "topics": ["data-loading", "batching"],
  "runtime": "local",
  "description": "Implement a `get_batch(data, block_size, batch_size)` function that creates a random batch of training examples from the data tensor.\n\nThe function should return a tuple `(x, y)` where:\n- `x` has shape `(batch_size, block_size)` — the input sequences\n- `y` has shape `(batch_size, block_size)` — the target sequences (shifted by one position)\n\nEach row in the batch should be a random contiguous chunk from `data`. The target `y` is always the input shifted one step to the right — for every position in `x`, the corresponding position in `y` is the next character in the sequence.",
  "starterCode": "import torch\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\ndef get_batch(data, block_size, batch_size):\n    \"\"\"\n    Generate a batch of inputs x and targets y from data.\n    \n    Args:\n        data: 1D tensor of encoded text\n        block_size: length of each sequence\n        batch_size: number of sequences in the batch\n    \n    Returns:\n        (x, y) where x and y each have shape (batch_size, block_size)\n    \"\"\"\n    # TODO: generate random starting indices for each sequence in the batch\n    # TODO: build x and y by slicing data at those offsets\n    # TODO: return (x, y)\n    pass\n",
  "solutionCode": "import torch\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\ndef get_batch(data, block_size, batch_size):\n    \"\"\"\n    Generate a batch of inputs x and targets y from data.\n    \n    Args:\n        data: 1D tensor of encoded text\n        block_size: length of each sequence\n        batch_size: number of sequences in the batch\n    \n    Returns:\n        (x, y) where x and y each have shape (batch_size, block_size)\n    \"\"\"\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n",
  "testCode": "try:\n    x, y = get_batch(data, block_size=8, batch_size=4)\n    assert x.shape == (4, 8), f\"Expected x shape (4, 8), got {x.shape}\"\n    assert y.shape == (4, 8), f\"Expected y shape (4, 8), got {y.shape}\"\n    print(\"PASS: x and y have correct shapes\")\nexcept Exception as e:\n    print(f\"FAIL: x and y have correct shapes — {e}\")\n\ntry:\n    x, y = get_batch(data, block_size=8, batch_size=4)\n    assert x.dtype == torch.long, f\"Expected dtype torch.long, got {x.dtype}\"\n    assert y.dtype == torch.long, f\"Expected dtype torch.long, got {y.dtype}\"\n    print(\"PASS: x and y have dtype torch.long\")\nexcept Exception as e:\n    print(f\"FAIL: x and y have dtype torch.long — {e}\")\n\ntry:\n    x, y = get_batch(data, block_size=8, batch_size=4)\n    for i in range(4):\n        for j in range(7):\n            assert y[i, j] == x[i, j+1], f\"y[{i},{j}] should equal x[{i},{j+1}]\"\n    print(\"PASS: y is x shifted by one position\")\nexcept Exception as e:\n    print(f\"FAIL: y is x shifted by one position — {e}\")\n\ntry:\n    x1, _ = get_batch(data, block_size=8, batch_size=4)\n    x2, _ = get_batch(data, block_size=8, batch_size=4)\n    assert not torch.equal(x1, x2), \"Two batches should (almost certainly) be different\"\n    print(\"PASS: batches are randomly sampled\")\nexcept Exception as e:\n    print(f\"FAIL: batches are randomly sampled — {e}\")\n",
  "hints": [
    "Use torch.randint(len(data) - block_size, (batch_size,)) to generate random starting positions.",
    "For each starting index i, the input is data[i:i+block_size] and the target is data[i+1:i+block_size+1].",
    "Use torch.stack() to combine the individual sequences into a batch tensor."
  ]
}