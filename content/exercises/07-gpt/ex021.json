{
  "id": "07-gpt-ex021",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg15",
  "title": "Assemble a Transformer Block",
  "difficulty": "hard",
  "order": 21,
  "runtime": "local",
  "topics": [
    "transformer-block",
    "architecture"
  ],
  "description": "Implement a complete Transformer `Block` module that combines all the components: multi-head self-attention, feed-forward network, residual connections, layer normalization, and dropout.\n\nYour class should:\n- Accept `n_embd`, `n_head`, `block_size`, and `dropout` as constructor arguments\n- Create all sub-components: multi-head attention, feed-forward, two layer norms, and a dropout layer\n- In `forward(self, x)`, apply the full block computation using pre-norm formulation with dropout on each sub-layer's output (before the residual addition)\n\nThis is the fundamental building block of the GPT architecture.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8, dropout=0.0):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8, dropout=0.0):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Your Implementation ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8, dropout=0.0):\n        super().__init__()\n        head_size = n_embd // n_head\n        # TODO: Create all sub-components:\n        #   self.sa - MultiHeadAttention\n        #   self.ffwd - FeedForward\n        #   self.ln1 - LayerNorm\n        #   self.ln2 - LayerNorm\n        pass\n\n    def forward(self, x):\n        # TODO: Full block with pre-norm, residual connections\n        # x = x + self.sa(self.ln1(x))\n        # x = x + self.ffwd(self.ln2(x))\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8, dropout=0.0):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8, dropout=0.0):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Solution ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=8, dropout=0.0):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nB, T, C = 2, 8, 32\nn_head = 4\ndropout = 0.1\nx = torch.randn(B, T, C)\nblock = Block(C, n_head, block_size=T, dropout=dropout)\n\ntry:\n    block.eval()\n    out = block(x)\n    assert out.shape == (B, T, C), f\"Expected shape {(B, T, C)}, got {out.shape}\"\n    print(f\"PASS: output shape is correct {out.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is correct — {e}\")\n\ntry:\n    assert hasattr(block, 'sa'), \"Block should have 'sa' (self-attention)\"\n    assert hasattr(block, 'ffwd'), \"Block should have 'ffwd' (feed-forward)\"\n    assert hasattr(block, 'ln1'), \"Block should have 'ln1' (layer norm 1)\"\n    assert hasattr(block, 'ln2'), \"Block should have 'ln2' (layer norm 2)\"\n    print(\"PASS: block has all required submodules (sa, ffwd, ln1, ln2)\")\nexcept Exception as e:\n    print(f\"FAIL: block has all required submodules — {e}\")\n\ntry:\n    assert isinstance(block.sa, MultiHeadAttention), \"sa should be MultiHeadAttention\"\n    assert isinstance(block.ffwd, FeedForward), \"ffwd should be FeedForward\"\n    assert isinstance(block.ln1, nn.LayerNorm), \"ln1 should be nn.LayerNorm\"\n    assert isinstance(block.ln2, nn.LayerNorm), \"ln2 should be nn.LayerNorm\"\n    print(\"PASS: all submodules have correct types\")\nexcept Exception as e:\n    print(f\"FAIL: all submodules have correct types — {e}\")\n\ntry:\n    has_dropout = False\n    for module in block.modules():\n        if isinstance(module, nn.Dropout):\n            has_dropout = True\n            break\n    assert has_dropout, \"Block should contain Dropout layers\"\n    print(\"PASS: dropout layer exists in the block\")\nexcept Exception as e:\n    print(f\"FAIL: dropout layer exists in the block — {e}\")\n\ntry:\n    total_params = sum(p.numel() for p in block.parameters())\n    assert total_params > 0, \"Block should have learnable parameters\"\n    print(f\"PASS: block has {total_params} learnable parameters\")\nexcept Exception as e:\n    print(f\"FAIL: block has learnable parameters — {e}\")\n",
  "hints": [
    "The complete block needs: MultiHeadAttention, FeedForward, two LayerNorms. Dropout is already built into the prerequisite classes.",
    "Pre-norm formulation: normalize first, then apply the sub-layer, then add the residual. x = x + self.sa(self.ln1(x)).",
    "head_size = n_embd // n_head ensures the concatenated output of all heads matches n_embd. Pass the dropout parameter to MultiHeadAttention and FeedForward."
  ]
}