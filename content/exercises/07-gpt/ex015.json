{
  "id": "07-gpt-ex015",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg11",
  "title": "Self-Attention Head as nn.Module",
  "difficulty": "hard",
  "order": 15,
  "runtime": "local",
  "topics": [
    "self-attention",
    "nn-module",
    "scaled-attention"
  ],
  "description": "Implement a `Head` class that inherits from `nn.Module` and encapsulates a single head of self-attention.\n\nThe class should:\n- Accept `head_size` in its constructor\n- Create learned linear projections for keys, queries, and values (without bias)\n- Register the lower-triangular causal mask as a **buffer** (not a parameter) so it moves with the model to different devices but is not updated by the optimizer\n- In the `forward` method, compute scaled dot-product attention with causal masking and return the aggregated output\n\nInput to `forward` is `x` of shape `(B, T, C)`. Output should be of shape `(B, T, head_size)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nB, T, C = 2, 4, 8\nhead_size = 4\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # TODO: create key, query, value nn.Linear layers (C -> head_size, no bias)\n        # TODO: register a buffer called 'tril' with the lower-triangular mask\n        pass\n    \n    def forward(self, x):\n        \"\"\"Compute self-attention.\n        \n        Args:\n            x: tensor of shape (B, T, C)\n        Returns:\n            out: tensor of shape (B, T, head_size)\n        \"\"\"\n        B, T, C = x.shape\n        # TODO: compute k, q, v from x\n        # TODO: compute attention scores with scaling\n        # TODO: apply causal mask using self.tril\n        # TODO: softmax and aggregate\n        pass\n\ntorch.manual_seed(42)\nhead = Head(head_size)\nx = torch.randn(B, T, C)\nout = head(x)\nprint(\"output shape:\", out.shape)\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nB, T, C = 2, 4, 8\nhead_size = 4\n\nclass Head(nn.Module):\n    \"\"\"One head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(C, head_size, bias=False)\n        self.query = nn.Linear(C, head_size, bias=False)\n        self.value = nn.Linear(C, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(T, T)))\n    \n    def forward(self, x):\n        \"\"\"Compute self-attention.\n        \n        Args:\n            x: tensor of shape (B, T, C)\n        Returns:\n            out: tensor of shape (B, T, head_size)\n        \"\"\"\n        B, T, C = x.shape\n        k = self.key(x)    # (B, T, head_size)\n        q = self.query(x)  # (B, T, head_size)\n        v = self.value(x)  # (B, T, head_size)\n        \n        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n        out = wei @ v  # (B, T, head_size)\n        return out\n\ntorch.manual_seed(42)\nhead = Head(head_size)\nx = torch.randn(B, T, C)\nout = head(x)\nprint(\"output shape:\", out.shape)\n",
  "testCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nB, T, C = 2, 4, 8\nhead_size = 4\n\ntorch.manual_seed(42)\nhead = Head(head_size)\nx = torch.randn(B, T, C)\nout = head(x)\n\ntry:\n    assert out.shape == (B, T, head_size)\n    print(\"PASS: output shape is (B, T, head_size)\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is (B, T, head_size) — {e}\")\n\ntry:\n    assert isinstance(head, nn.Module)\n    print(\"PASS: Head is an nn.Module subclass\")\nexcept Exception as e:\n    print(f\"FAIL: Head is an nn.Module subclass — {e}\")\n\ntry:\n    param_names = [name for name, _ in head.named_parameters()]\n    assert 'key.weight' in param_names\n    assert 'query.weight' in param_names\n    assert 'value.weight' in param_names\n    print(\"PASS: module has key, query, value parameters\")\nexcept Exception as e:\n    print(f\"FAIL: module has key, query, value parameters — {e}\")\n\ntry:\n    buffer_names = [name for name, _ in head.named_buffers()]\n    assert 'tril' in buffer_names\n    print(\"PASS: tril is registered as a buffer\")\nexcept Exception as e:\n    print(f\"FAIL: tril is registered as a buffer — {e}\")\n\ntry:\n    assert 'tril' not in [name for name, _ in head.named_parameters()]\n    print(\"PASS: tril is not a learnable parameter\")\nexcept Exception as e:\n    print(f\"FAIL: tril is not a learnable parameter — {e}\")\n\ntry:\n    # Test causal masking: perturb future tokens, check past is unchanged\n    x2 = x.clone()\n    x2[:, -1, :] = torch.randn(B, C) * 100\n    out2 = head(x2)\n    assert torch.allclose(out[:, 0, :], out2[:, 0, :], atol=1e-5)\n    print(\"PASS: causal masking works (future does not affect past)\")\nexcept Exception as e:\n    print(f\"FAIL: causal masking works (future does not affect past) — {e}\")\n",
  "hints": [
    "In __init__, create three nn.Linear(C, head_size, bias=False) layers and use self.register_buffer('tril', torch.tril(torch.ones(T, T))) to store the mask.",
    "In forward, compute k, q, v by passing x through the linear layers. Then compute wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 for scaled attention.",
    "Apply the mask with wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')), then F.softmax(wei, dim=-1), then return wei @ v. The [:T, :T] slicing lets it work with variable sequence lengths."
  ]
}