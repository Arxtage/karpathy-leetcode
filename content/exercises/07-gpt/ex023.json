{
  "id": "07-gpt-ex023",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg16",
  "title": "Train GPT on Shakespeare",
  "difficulty": "hard",
  "order": 23,
  "runtime": "local",
  "topics": [
    "training",
    "gpt",
    "shakespeare"
  ],
  "description": "Put everything together: build a GPT language model, train it on a small Shakespeare text excerpt, and generate new text.\n\nYou need to implement:\n1. **Data encoding**: create `encode` and `decode` functions using character-level tokenization from the provided text\n2. **`get_batch` function**: randomly sample a batch of input-target pairs from the training data\n3. **Model instantiation**: create a `GPTLanguageModel` with the given hyperparameters\n4. **Training loop**: train the model for `max_iters` steps using AdamW optimizer\n5. **Text generation**: generate new text from the trained model\n\nAll component classes (Head, MultiHeadAttention, FeedForward, Block, GPTLanguageModel) are provided. The training text is provided as a variable. Your job is to wire everything together and make the model learn.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd, block_size, dropout):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        return wei @ v\n\n# --- Prerequisite: Multi-Head Attention ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(out))\n\n# --- Prerequisite: Feed-Forward ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Prerequisite: Transformer Block ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size, dropout):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# --- Prerequisite: GPT Language Model ---\nclass GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n        super().__init__()\n        self.block_size = block_size\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# --- Training text ---\ntext = \"\"\"ROMEO:\nIs the day so young?\n\nBENVOLIO:\nBut new struck nine.\n\nROMEO:\nAy me! sad hours seem long.\nWas that my father that went hence so fast?\n\nBENVOLIO:\nIt was. What sadness lengthens Romeo's hours?\n\nROMEO:\nNot having that, which, having, makes them short.\n\nBENVOLIO:\nIn love?\n\nROMEO:\nOut--\n\nBENVOLIO:\nOf love?\n\nROMEO:\nOut of her favour, where I am in love.\n\nBENVOLIO:\nAlas, that love, so gentle in his view,\nShould be so tyrannous and rough in proof!\n\nROMEO:\nAlas, that love, whose view is muffled still,\nShould, without eyes, see pathways to his will!\nWhere shall we dine? O me! What fray was here?\nYet tell me not, for I have heard it all.\nHere's much to do with hate, but more with love.\nWhy, then, O brawling love! O loving hate!\nO any thing, of nothing first create!\nO heavy lightness! serious vanity!\nMis-shapen chaos of well-seeming forms!\nFeather of lead, bright smoke, cold fire, sick health!\nStill-waking sleep, that is not what it is!\nThis love feel I, that feel no love in this.\nDost thou not laugh?\n\nBENVOLIO:\nNo, coz, I rather weep.\n\nROMEO:\nGood heart, at what?\n\nBENVOLIO:\nAt thy good heart's oppression.\n\nROMEO:\nWhy, such is love's transgression.\nGriefs of mine own lie heavy in my breast,\nWhich thou wilt propagate, to have it prest\nWith more of thine: this love that thou hast shown\nDoth add more grief to too much of mine own.\nLove is a smoke raised with the fume of sighs;\nBeing purged, a fire sparkling in lovers' eyes;\nBeing vex'd a sea nourish'd with lovers' tears:\nWhat is it else? a madness most discreet,\nA choking gall and a preserving sweet.\nFarewell, my coz.\n\nBENVOLIO:\nSoft! I will go along;\nAn if you leave me so, you do me wrong.\n\nROMEO:\nTut, I have lost myself; I am not here;\nThis is not Romeo, he is some where else.\n\nBENVOLIO:\nTell me in sadness, who is that you love.\n\nROMEO:\nWhat, shall I groan and tell thee?\n\nBENVOLIO:\nGroan! why, no. But sadly tell me who.\n\nROMEO:\nBid a sick man in sadness make his will:\nAh, word ill urged to one that is so ill!\nIn sadness, cousin, I do love a woman.\n\nBENVOLIO:\nI aim'd so near, when I supposed you loved.\n\nROMEO:\nA right good mark-man! And she's fair I love.\n\"\"\"\n\n# --- Hyperparameters ---\nbatch_size = 32\nblock_size = 64\nn_embd = 64\nn_head = 4\nn_layer = 3\ndropout = 0.0\nlearning_rate = 1e-3\nmax_iters = 500\n\ntorch.manual_seed(1337)\n\n# TODO: Step 1 - Create encode/decode functions\n# Build a character-level vocabulary from the text\n# encode: string -> list of integers\n# decode: list of integers -> string\n\n# TODO: Step 2 - Encode the text and create a tensor\n# data = torch.tensor(encode(text), dtype=torch.long)\n\n# TODO: Step 3 - Implement get_batch(data, batch_size, block_size)\n# Randomly sample batch_size starting positions\n# Return (x, y) where x is input and y is target (shifted by 1)\n\n# TODO: Step 4 - Create the model\n# model = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n\n# TODO: Step 5 - Create optimizer and run training loop\n# Use AdamW optimizer with the given learning_rate\n# For max_iters steps: get a batch, compute loss, backprop, update\n\n# TODO: Step 6 - Generate text\n# context = torch.zeros((1, 1), dtype=torch.long)\n# generated_ids = model.generate(context, max_new_tokens=200)\n# generated_text = decode(generated_ids[0].tolist())\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd, block_size, dropout):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        return wei @ v\n\n# --- Prerequisite: Multi-Head Attention ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd, block_size, dropout):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return self.dropout(self.proj(out))\n\n# --- Prerequisite: Feed-Forward ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Prerequisite: Transformer Block ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size, dropout):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# --- Prerequisite: GPT Language Model ---\nclass GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n        super().__init__()\n        self.block_size = block_size\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n# --- Training text ---\ntext = \"\"\"ROMEO:\nIs the day so young?\n\nBENVOLIO:\nBut new struck nine.\n\nROMEO:\nAy me! sad hours seem long.\nWas that my father that went hence so fast?\n\nBENVOLIO:\nIt was. What sadness lengthens Romeo's hours?\n\nROMEO:\nNot having that, which, having, makes them short.\n\nBENVOLIO:\nIn love?\n\nROMEO:\nOut--\n\nBENVOLIO:\nOf love?\n\nROMEO:\nOut of her favour, where I am in love.\n\nBENVOLIO:\nAlas, that love, so gentle in his view,\nShould be so tyrannous and rough in proof!\n\nROMEO:\nAlas, that love, whose view is muffled still,\nShould, without eyes, see pathways to his will!\nWhere shall we dine? O me! What fray was here?\nYet tell me not, for I have heard it all.\nHere's much to do with hate, but more with love.\nWhy, then, O brawling love! O loving hate!\nO any thing, of nothing first create!\nO heavy lightness! serious vanity!\nMis-shapen chaos of well-seeming forms!\nFeather of lead, bright smoke, cold fire, sick health!\nStill-waking sleep, that is not what it is!\nThis love feel I, that feel no love in this.\nDost thou not laugh?\n\nBENVOLIO:\nNo, coz, I rather weep.\n\nROMEO:\nGood heart, at what?\n\nBENVOLIO:\nAt thy good heart's oppression.\n\nROMEO:\nWhy, such is love's transgression.\nGriefs of mine own lie heavy in my breast,\nWhich thou wilt propagate, to have it prest\nWith more of thine: this love that thou hast shown\nDoth add more grief to too much of mine own.\nLove is a smoke raised with the fume of sighs;\nBeing purged, a fire sparkling in lovers' eyes;\nBeing vex'd a sea nourish'd with lovers' tears:\nWhat is it else? a madness most discreet,\nA choking gall and a preserving sweet.\nFarewell, my coz.\n\nBENVOLIO:\nSoft! I will go along;\nAn if you leave me so, you do me wrong.\n\nROMEO:\nTut, I have lost myself; I am not here;\nThis is not Romeo, he is some where else.\n\nBENVOLIO:\nTell me in sadness, who is that you love.\n\nROMEO:\nWhat, shall I groan and tell thee?\n\nBENVOLIO:\nGroan! why, no. But sadly tell me who.\n\nROMEO:\nBid a sick man in sadness make his will:\nAh, word ill urged to one that is so ill!\nIn sadness, cousin, I do love a woman.\n\nBENVOLIO:\nI aim'd so near, when I supposed you loved.\n\nROMEO:\nA right good mark-man! And she's fair I love.\n\"\"\"\n\n# --- Hyperparameters ---\nbatch_size = 32\nblock_size = 64\nn_embd = 64\nn_head = 4\nn_layer = 3\ndropout = 0.0\nlearning_rate = 1e-3\nmax_iters = 500\n\ntorch.manual_seed(1337)\n\n# Step 1 - Create encode/decode functions\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\nitos = {i: ch for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\n# Step 2 - Encode the text and create a tensor\ndata = torch.tensor(encode(text), dtype=torch.long)\n\n# Step 3 - get_batch function\ndef get_batch(data, batch_size, block_size):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\n# Step 4 - Create the model\nmodel = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size, dropout)\n\n# Step 5 - Training loop\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n    xb, yb = get_batch(data, batch_size, block_size)\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nfinal_loss = loss.item()\n\n# Step 6 - Generate text\ncontext = torch.zeros((1, 1), dtype=torch.long)\ngenerated_ids = model.generate(context, max_new_tokens=200)\ngenerated_text = decode(generated_ids[0].tolist())\n",
  "testCode": "try:\n    assert final_loss < 3.0, f\"Expected final loss < 3.0, got {final_loss:.4f}\"\n    print(f\"PASS: final loss is {final_loss:.4f} (< 3.0)\")\nexcept Exception as e:\n    print(f\"FAIL: final loss < 3.0 — {e}\")\n\ntry:\n    assert len(generated_text) > 0, \"Generated text should be non-empty\"\n    print(f\"PASS: generated text is non-empty ({len(generated_text)} chars)\")\nexcept Exception as e:\n    print(f\"FAIL: generated text is non-empty — {e}\")\n\ntry:\n    valid_chars = set(text)\n    invalid = [c for c in generated_text if c not in valid_chars]\n    assert len(invalid) == 0, f\"Generated text contains invalid characters: {set(invalid)}\"\n    print(\"PASS: all generated characters are valid (from training vocabulary)\")\nexcept Exception as e:\n    print(f\"FAIL: all generated characters are valid — {e}\")\n\ntry:\n    assert vocab_size == len(chars), f\"vocab_size should match number of unique chars\"\n    assert len(data) > 0, \"Encoded data should be non-empty\"\n    assert data.dtype == torch.long, \"Data should be long tensor\"\n    print(f\"PASS: data encoding correct (vocab_size={vocab_size}, data_len={len(data)})\")\nexcept Exception as e:\n    print(f\"FAIL: data encoding correct — {e}\")\n\ntry:\n    xb, yb = get_batch(data, batch_size, block_size)\n    assert xb.shape == (batch_size, block_size), f\"Expected batch shape ({batch_size}, {block_size}), got {xb.shape}\"\n    assert yb.shape == (batch_size, block_size), f\"Expected target shape ({batch_size}, {block_size}), got {yb.shape}\"\n    print(f\"PASS: get_batch returns correct shapes {xb.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: get_batch returns correct shapes — {e}\")\n",
  "hints": [
    "For character-level encoding: get sorted unique characters from text, create char-to-int (stoi) and int-to-char (itos) mappings.",
    "get_batch: use torch.randint to pick random starting positions, then slice data[i:i+block_size] for inputs and data[i+1:i+block_size+1] for targets.",
    "Training loop: for each iteration, get a batch, compute loss with model(xb, yb), zero gradients, call loss.backward(), then optimizer.step()."
  ]
}