{
  "id": "07-gpt-ex017",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg12",
  "title": "Multi-Head Attention with Projection",
  "difficulty": "medium",
  "order": 17,
  "runtime": "local",
  "topics": [
    "multi-head-attention",
    "projection"
  ],
  "description": "Extend the `MultiHeadAttention` module to include an output projection layer.\n\nAfter concatenating the outputs of all attention heads, apply a linear projection that maps the concatenated output back to the embedding dimension `n_embd`.\n\nYour class should:\n- Accept `num_heads`, `head_size`, and `n_embd` as constructor arguments\n- Create the attention heads using `nn.ModuleList`\n- Create a projection layer `self.proj` as `nn.Linear`\n- In `forward`, concatenate head outputs, then pass through the projection layer\n\nThe output shape should be `(B, T, n_embd)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Your Implementation ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        # TODO: Create a ModuleList of Head modules\n        # TODO: Create a projection linear layer (head_size * num_heads -> n_embd)\n        pass\n\n    def forward(self, x):\n        # TODO: Run all heads, concatenate, then apply projection\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Solution ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=8):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nB, T, C = 2, 8, 32\nnum_heads = 4\nhead_size = 8\nx = torch.randn(B, T, C)\nmha = MultiHeadAttention(num_heads, head_size, n_embd=C, block_size=T)\n\ntry:\n    out = mha(x)\n    assert out.shape == (B, T, C), f\"Expected shape {(B, T, C)}, got {out.shape}\"\n    print(f\"PASS: output shape is (B, T, n_embd) = {out.shape}\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is (B, T, n_embd) — {e}\")\n\ntry:\n    assert hasattr(mha, 'proj'), \"Should have a 'proj' attribute\"\n    assert isinstance(mha.proj, nn.Linear), \"'proj' should be an nn.Linear\"\n    print(\"PASS: projection layer exists and is nn.Linear\")\nexcept Exception as e:\n    print(f\"FAIL: projection layer exists and is nn.Linear — {e}\")\n\ntry:\n    assert mha.proj.in_features == num_heads * head_size, f\"proj input should be {num_heads * head_size}, got {mha.proj.in_features}\"\n    assert mha.proj.out_features == C, f\"proj output should be {C}, got {mha.proj.out_features}\"\n    print(f\"PASS: projection dimensions are correct ({num_heads * head_size} -> {C})\")\nexcept Exception as e:\n    print(f\"FAIL: projection dimensions are correct — {e}\")\n\ntry:\n    out = mha(x)\n    assert not torch.isnan(out).any(), \"Output contains NaN values\"\n    total_params = sum(p.numel() for p in mha.parameters())\n    assert total_params > 0, \"Model should have parameters\"\n    print(f\"PASS: output is valid, model has {total_params} parameters\")\nexcept Exception as e:\n    print(f\"FAIL: output is valid — {e}\")\n",
  "hints": [
    "The projection layer maps from the concatenated head dimension (head_size * num_heads) back to n_embd.",
    "In forward: first concatenate all head outputs with torch.cat(..., dim=-1), then apply self.proj to the result.",
    "nn.Linear(head_size * num_heads, n_embd) is the projection that lets the model mix information from all heads."
  ]
}