{
  "id": "07-gpt-ex022",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg15",
  "title": "Full GPT Language Model",
  "difficulty": "medium",
  "order": 22,
  "runtime": "local",
  "topics": [
    "gpt",
    "language-model",
    "transformer"
  ],
  "description": "Implement a complete `GPTLanguageModel` module that assembles all components into a working language model.\n\nYour class should have:\n- Token embedding table and positional embedding table\n- A stack of Transformer blocks followed by a final layer norm\n- A linear head that maps from the embedding dimension to the vocabulary size\n- A `forward(self, idx, targets=None)` method that returns logits and an optional loss (cross-entropy loss when targets are provided, `None` otherwise)\n- A `generate(self, idx, max_new_tokens)` method that autoregressively generates new tokens by sampling from the model's output distribution\n\nUse small hyperparameters: `vocab_size=26`, `n_embd=32`, `n_head=4`, `n_layer=2`, `block_size=16`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=16, dropout=0.0):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=16, dropout=0.0):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Prerequisite: Transformer Block ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=16, dropout=0.0):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# --- Your Implementation ---\nclass GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size=26, n_embd=32, n_head=4, n_layer=2, block_size=16, dropout=0.0):\n        super().__init__()\n        self.block_size = block_size\n        # TODO: Create the following layers:\n        #   self.token_embedding_table - nn.Embedding(vocab_size, n_embd)\n        #   self.position_embedding_table - nn.Embedding(block_size, n_embd)\n        #   self.blocks - nn.Sequential of Block modules\n        #   self.ln_f - final LayerNorm\n        #   self.lm_head - nn.Linear(n_embd, vocab_size)\n        pass\n\n    def forward(self, idx, targets=None):\n        # idx shape: (B, T) — integer token indices\n        # TODO:\n        #   1. Get token embeddings and positional embeddings\n        #   2. Add them together\n        #   3. Pass through transformer blocks\n        #   4. Apply final layer norm\n        #   5. Apply lm_head to get logits (B, T, vocab_size)\n        #   6. If targets is not None, compute cross-entropy loss\n        #   7. Return (logits, loss) where loss is None if no targets\n        pass\n\n    def generate(self, idx, max_new_tokens):\n        # idx shape: (B, T) — current context\n        # TODO:\n        #   For each new token:\n        #   1. Crop idx to last block_size tokens\n        #   2. Get logits from forward pass\n        #   3. Take logits at the last time step\n        #   4. Apply softmax to get probabilities\n        #   5. Sample from the distribution\n        #   6. Append to idx\n        #   Return the extended idx\n        pass\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Prerequisite: Single Head of Self-Attention ---\nclass Head(nn.Module):\n    def __init__(self, head_size, n_embd=32, block_size=16, dropout=0.0):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n# --- Prerequisite: Multi-Head Attention with Projection ---\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size, n_embd=32, block_size=16, dropout=0.0):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size, n_embd, block_size, dropout) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n# --- Prerequisite: Feed-Forward Network ---\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd, dropout=0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Prerequisite: Transformer Block ---\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head, block_size=16, dropout=0.0):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size, n_embd, block_size, dropout)\n        self.ffwd = FeedForward(n_embd, dropout)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# --- Solution ---\nclass GPTLanguageModel(nn.Module):\n    def __init__(self, vocab_size=26, n_embd=32, n_head=4, n_layer=2, block_size=16, dropout=0.0):\n        super().__init__()\n        self.block_size = block_size\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head, block_size, dropout) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits_flat = logits.view(B * T, C)\n            targets_flat = targets.view(B * T)\n            loss = F.cross_entropy(logits_flat, targets_flat)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n",
  "testCode": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nvocab_size = 26\nn_embd = 32\nn_head = 4\nn_layer = 2\nblock_size = 16\nB, T = 2, 8\n\nmodel = GPTLanguageModel(vocab_size, n_embd, n_head, n_layer, block_size)\nmodel.eval()\n\ntry:\n    idx = torch.randint(0, vocab_size, (B, T))\n    logits, loss = model(idx)\n    assert logits.shape == (B, T, vocab_size), f\"Expected logits shape {(B, T, vocab_size)}, got {logits.shape}\"\n    assert loss is None, \"Loss should be None when no targets provided\"\n    print(f\"PASS: forward returns correct logits shape {logits.shape} and loss=None\")\nexcept Exception as e:\n    print(f\"FAIL: forward returns correct logits shape and loss=None — {e}\")\n\ntry:\n    idx = torch.randint(0, vocab_size, (B, T))\n    targets = torch.randint(0, vocab_size, (B, T))\n    logits, loss = model(idx, targets)\n    assert loss is not None, \"Loss should not be None when targets provided\"\n    assert loss.dim() == 0, f\"Loss should be scalar, got shape {loss.shape}\"\n    assert loss.item() > 0, \"Loss should be positive\"\n    print(f\"PASS: loss is a positive scalar ({loss.item():.4f}) when targets provided\")\nexcept Exception as e:\n    print(f\"FAIL: loss is a positive scalar when targets provided — {e}\")\n\ntry:\n    idx = torch.randint(0, vocab_size, (1, 4))\n    max_new = 10\n    generated = model.generate(idx, max_new)\n    assert generated.shape == (1, 4 + max_new), f\"Expected shape (1, {4 + max_new}), got {generated.shape}\"\n    assert (generated >= 0).all() and (generated < vocab_size).all(), \"Generated tokens should be valid indices\"\n    print(f\"PASS: generate extends sequence from length 4 to {generated.shape[1]}\")\nexcept Exception as e:\n    print(f\"FAIL: generate extends sequence — {e}\")\n\ntry:\n    assert hasattr(model, 'token_embedding_table'), \"Should have token_embedding_table\"\n    assert hasattr(model, 'position_embedding_table'), \"Should have position_embedding_table\"\n    assert hasattr(model, 'blocks'), \"Should have blocks\"\n    assert hasattr(model, 'ln_f'), \"Should have ln_f\"\n    assert hasattr(model, 'lm_head'), \"Should have lm_head\"\n    print(\"PASS: model has all required components\")\nexcept Exception as e:\n    print(f\"FAIL: model has all required components — {e}\")\n",
  "hints": [
    "Token embeddings map token indices to vectors. Positional embeddings map position indices (0, 1, ..., T-1) to vectors. Add them together before the transformer blocks.",
    "For the loss, reshape logits to (B*T, vocab_size) and targets to (B*T) before calling F.cross_entropy.",
    "In generate, crop the context to the last block_size tokens, get logits for the last position only, apply softmax, and sample with torch.multinomial."
  ]
}