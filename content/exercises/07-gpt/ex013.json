{
  "id": "07-gpt-ex013",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg09",
  "title": "Masked Self-Attention (Position-Based)",
  "difficulty": "medium",
  "order": 13,
  "runtime": "local",
  "topics": [
    "self-attention",
    "causal-masking"
  ],
  "description": "Implement a function that applies the full **causal (masked) attention pattern** to aggregate information from past tokens.\n\nGiven an input tensor `x` of shape `(B, T, C)` and an arbitrary weight matrix `wei` of shape `(B, T, T)`, apply a lower-triangular causal mask so that position `t` can only attend to positions `0..t`. After masking, apply softmax to get valid attention weights, then use these weights to aggregate information from `x`.\n\nThe weight matrix `wei` starts as all zeros (meaning uniform attention before masking/softmax), but the function should work correctly for any input weight matrix -- the causal mask must always zero out future positions.",
  "starterCode": "import torch\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nx = torch.randn(B, T, C)\n\ndef masked_attention(x, wei):\n    \"\"\"Apply causal mask to weight matrix, softmax, and aggregate.\n    \n    Args:\n        x: tensor of shape (B, T, C)\n        wei: tensor of shape (B, T, T) — raw attention scores\n    Returns:\n        out: tensor of shape (B, T, C)\n    \"\"\"\n    B, T, C = x.shape\n    # TODO: create a lower-triangular mask of shape (T, T)\n    # TODO: use masked_fill to set upper-triangle positions of wei to -inf\n    # TODO: apply softmax along the last dimension\n    # TODO: multiply the weights by x to get the output\n    pass\n\n# Start with uniform weights (zeros)\nwei = torch.zeros((B, T, T))\nout = masked_attention(x, wei)\nprint(\"output shape:\", out.shape)\n",
  "solutionCode": "import torch\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nx = torch.randn(B, T, C)\n\ndef masked_attention(x, wei):\n    \"\"\"Apply causal mask to weight matrix, softmax, and aggregate.\n    \n    Args:\n        x: tensor of shape (B, T, C)\n        wei: tensor of shape (B, T, T) — raw attention scores\n    Returns:\n        out: tensor of shape (B, T, C)\n    \"\"\"\n    B, T, C = x.shape\n    tril = torch.tril(torch.ones(T, T))\n    wei = wei.masked_fill(tril == 0, float('-inf'))\n    wei = F.softmax(wei, dim=-1)\n    out = wei @ x\n    return out\n\n# Start with uniform weights (zeros)\nwei = torch.zeros((B, T, T))\nout = masked_attention(x, wei)\nprint(\"output shape:\", out.shape)\n",
  "testCode": "import torch\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nx = torch.randn(B, T, C)\n\n# Test with uniform weights\nwei_uniform = torch.zeros((B, T, T))\nout = masked_attention(x, wei_uniform)\n\ntry:\n    assert out.shape == (B, T, C)\n    print(\"PASS: output shape matches input shape (B, T, C)\")\nexcept Exception as e:\n    print(f\"FAIL: output shape matches input shape (B, T, C) — {e}\")\n\ntry:\n    # With uniform weights, result should match cumulative averaging\n    xbow_ref = torch.zeros((B, T, C))\n    for b in range(B):\n        for t in range(T):\n            xbow_ref[b, t] = x[b, :t+1].mean(dim=0)\n    assert torch.allclose(out, xbow_ref, atol=1e-6)\n    print(\"PASS: uniform weights produce cumulative average\")\nexcept Exception as e:\n    print(f\"FAIL: uniform weights produce cumulative average — {e}\")\n\ntry:\n    # Future tokens should not influence past: perturb future and check past unchanged\n    x2 = x.clone()\n    x2[:, -1, :] = 999.0  # change last token\n    out2 = masked_attention(x2, wei_uniform)\n    assert torch.allclose(out2[:, 0, :], out[:, 0, :], atol=1e-6)\n    print(\"PASS: future tokens do not influence past positions\")\nexcept Exception as e:\n    print(f\"FAIL: future tokens do not influence past positions — {e}\")\n\ntry:\n    # Test with random weights — just check shape and causality\n    wei_rand = torch.randn((B, T, T))\n    out_rand = masked_attention(x, wei_rand)\n    assert out_rand.shape == (B, T, C)\n    # First position should only depend on itself\n    x3 = x.clone()\n    x3[:, 1:, :] = torch.randn(B, T-1, C)\n    out3 = masked_attention(x3, wei_rand)\n    assert torch.allclose(out3[:, 0, :], out_rand[:, 0, :], atol=1e-6)\n    print(\"PASS: works with arbitrary weight matrices and preserves causality\")\nexcept Exception as e:\n    print(f\"FAIL: works with arbitrary weight matrices and preserves causality — {e}\")\n",
  "hints": [
    "The causal mask is a lower-triangular matrix: torch.tril(torch.ones(T, T)). Positions where the mask is 0 are future positions that must be blocked.",
    "Use wei.masked_fill(tril == 0, float('-inf')) to fill future positions with -inf. This works even when wei has a batch dimension because the (T, T) mask broadcasts.",
    "After masking, apply F.softmax(wei, dim=-1) to normalize each row into a probability distribution, then multiply wei @ x to aggregate."
  ]
}