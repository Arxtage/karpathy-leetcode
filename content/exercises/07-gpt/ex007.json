{
  "id": "07-gpt-ex007",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg05",
  "title": "Bigram Language Model",
  "difficulty": "medium",
  "order": 7,
  "topics": ["language-models", "bigram", "cross-entropy"],
  "runtime": "local",
  "description": "Implement a `BigramLanguageModel` as an `nn.Module`.\n\nThe model should have:\n- An `__init__` method that creates a token embedding table mapping each token to a vector of size `vocab_size`.\n- A `forward(idx, targets=None)` method that:\n  - Looks up logits from the embedding table using `idx`.\n  - If `targets` is provided, computes the cross-entropy loss between the predicted logits and the targets, and returns `(logits, loss)`.\n  - If `targets` is `None`, returns `(logits, None)`.\n\nNote: `F.cross_entropy` expects the logits in shape `(N, C)` and targets in shape `(N,)`, so you will need to reshape appropriately.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # TODO: create a token embedding table of shape (vocab_size, vocab_size)\n        pass\n\n    def forward(self, idx, targets=None):\n        \"\"\"\n        Args:\n            idx: (B, T) tensor of token indices\n            targets: (B, T) tensor of target token indices, or None\n        Returns:\n            logits: (B, T, C) tensor of logits\n            loss: scalar loss if targets provided, else None\n        \"\"\"\n        # TODO: get logits from embedding table\n        # TODO: if targets given, reshape and compute cross_entropy loss\n        # TODO: return (logits, loss)\n        pass\n\nmodel = BigramLanguageModel(vocab_size)\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        \"\"\"\n        Args:\n            idx: (B, T) tensor of token indices\n            targets: (B, T) tensor of target token indices, or None\n        Returns:\n            logits: (B, T, C) tensor of logits\n            loss: scalar loss if targets provided, else None\n        \"\"\"\n        logits = self.token_embedding_table(idx)  # (B, T, C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits_reshaped = logits.view(B * T, C)\n            targets_reshaped = targets.view(B * T)\n            loss = F.cross_entropy(logits_reshaped, targets_reshaped)\n\n        return logits, loss\n\nmodel = BigramLanguageModel(vocab_size)\n",
  "testCode": "try:\n    ix = torch.randint(len(data) - 8, (4,))\n    xb = torch.stack([data[i:i+8] for i in ix])\n    yb = torch.stack([data[i+1:i+1+8] for i in ix])\n    logits, loss = model(xb, yb)\n    assert logits.shape == (4, 8, vocab_size), f\"Expected logits shape (4, 8, {vocab_size}), got {logits.shape}\"\n    print(\"PASS: logits shape is (B, T, vocab_size)\")\nexcept Exception as e:\n    print(f\"FAIL: logits shape is (B, T, vocab_size) — {e}\")\n\ntry:\n    ix = torch.randint(len(data) - 8, (4,))\n    xb = torch.stack([data[i:i+8] for i in ix])\n    yb = torch.stack([data[i+1:i+1+8] for i in ix])\n    logits, loss = model(xb, yb)\n    assert loss is not None, \"loss should not be None when targets are provided\"\n    assert loss.dim() == 0, f\"loss should be a scalar, got dim={loss.dim()}\"\n    print(\"PASS: loss is a scalar when targets are provided\")\nexcept Exception as e:\n    print(f\"FAIL: loss is a scalar when targets are provided — {e}\")\n\ntry:\n    ix = torch.randint(len(data) - 8, (4,))\n    xb = torch.stack([data[i:i+8] for i in ix])\n    logits, loss = model(xb)\n    assert loss is None, \"loss should be None when no targets provided\"\n    print(\"PASS: loss is None when no targets given\")\nexcept Exception as e:\n    print(f\"FAIL: loss is None when no targets given — {e}\")\n\ntry:\n    import math\n    ix = torch.randint(len(data) - 8, (4,))\n    xb = torch.stack([data[i:i+8] for i in ix])\n    yb = torch.stack([data[i+1:i+1+8] for i in ix])\n    logits, loss = model(xb, yb)\n    expected_loss = -math.log(1.0 / vocab_size)\n    assert abs(loss.item() - expected_loss) < 1.0, f\"Initial loss {loss.item():.2f} should be near {expected_loss:.2f} (within 1.0)\"\n    print(f\"PASS: initial loss ({loss.item():.2f}) is near -ln(1/{vocab_size}) = {expected_loss:.2f}\")\nexcept Exception as e:\n    print(f\"FAIL: initial loss is near -ln(1/vocab_size) — {e}\")\n",
  "hints": [
    "The embedding table is nn.Embedding(vocab_size, vocab_size) — it maps each token to a vector of logits over the vocabulary.",
    "F.cross_entropy expects inputs of shape (N, C) and targets of shape (N,). Reshape logits from (B, T, C) to (B*T, C) and targets from (B, T) to (B*T).",
    "Use logits.view(B*T, C) to reshape. Get B, T, C from logits.shape."
  ]
}