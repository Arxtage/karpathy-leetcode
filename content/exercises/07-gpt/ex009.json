{
  "id": "07-gpt-ex009",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg07",
  "title": "Train the Bigram Model",
  "difficulty": "medium",
  "order": 9,
  "topics": ["training-loop", "optimization"],
  "runtime": "local",
  "description": "Implement a training loop for the Bigram Language Model.\n\nYou are given a model and a `get_batch` function. Your task is to:\n\n1. Create an AdamW optimizer for the model's parameters with a learning rate of `1e-3`.\n2. Run a training loop for `num_steps` iterations. In each iteration:\n   - Get a batch of training data using `get_batch`.\n   - Compute the forward pass to get logits and loss.\n   - Zero the gradients, compute the backward pass, and update the parameters.\n3. Store the very first loss value in `initial_loss` and the very last loss value in `final_loss`.\n\nAfter training, `final_loss` should be lower than `initial_loss`, showing that the model has learned something.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\ndef get_batch(data, block_size=8, batch_size=4):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            logits = logits.view(B, T, -1)\n        return logits, loss\n\nmodel = BigramLanguageModel(vocab_size)\nnum_steps = 100\n\n# TODO: create an AdamW optimizer with lr=1e-3\noptimizer = None\n\n# TODO: implement the training loop\n# Store the first loss in initial_loss and the last loss in final_loss\ninitial_loss = None\nfinal_loss = None\n\n# for step in range(num_steps):\n#     ... get batch, forward, backward, step ...\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\ndef get_batch(data, block_size=8, batch_size=4):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n            logits = logits.view(B, T, -1)\n        return logits, loss\n\nmodel = BigramLanguageModel(vocab_size)\nnum_steps = 100\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n\ninitial_loss = None\nfinal_loss = None\n\nfor step in range(num_steps):\n    xb, yb = get_batch(data)\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    if step == 0:\n        initial_loss = loss.item()\n    if step == num_steps - 1:\n        final_loss = loss.item()\n",
  "testCode": "try:\n    assert optimizer is not None, \"optimizer should be created\"\n    assert isinstance(optimizer, torch.optim.AdamW), f\"Expected AdamW, got {type(optimizer)}\"\n    print(\"PASS: optimizer is an AdamW instance\")\nexcept Exception as e:\n    print(f\"FAIL: optimizer is an AdamW instance — {e}\")\n\ntry:\n    assert initial_loss is not None, \"initial_loss should be set\"\n    assert final_loss is not None, \"final_loss should be set\"\n    assert isinstance(initial_loss, float), \"initial_loss should be a float\"\n    assert isinstance(final_loss, float), \"final_loss should be a float\"\n    print(\"PASS: initial_loss and final_loss are set\")\nexcept Exception as e:\n    print(f\"FAIL: initial_loss and final_loss are set — {e}\")\n\ntry:\n    assert final_loss < initial_loss, f\"final_loss ({final_loss:.4f}) should be less than initial_loss ({initial_loss:.4f})\"\n    print(f\"PASS: loss decreased from {initial_loss:.4f} to {final_loss:.4f}\")\nexcept Exception as e:\n    print(f\"FAIL: loss decreased after training — {e}\")\n\ntry:\n    import math\n    expected_initial = -math.log(1.0 / vocab_size)\n    assert abs(initial_loss - expected_initial) < 1.5, f\"Initial loss {initial_loss:.2f} should be near {expected_initial:.2f}\"\n    print(f\"PASS: initial loss ({initial_loss:.2f}) is reasonable (near -ln(1/{vocab_size}) = {expected_initial:.2f})\")\nexcept Exception as e:\n    print(f\"FAIL: initial loss is reasonable — {e}\")\n",
  "hints": [
    "Create the optimizer: optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3).",
    "In the loop: get a batch, do a forward pass to get loss, call optimizer.zero_grad(), loss.backward(), optimizer.step().",
    "Store loss.item() (not the tensor) as initial_loss on the first step and final_loss on the last step."
  ]
}