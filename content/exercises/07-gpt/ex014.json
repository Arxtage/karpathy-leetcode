{
  "id": "07-gpt-ex014",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg10",
  "title": "Single-Head Self-Attention",
  "difficulty": "medium",
  "order": 14,
  "runtime": "local",
  "topics": [
    "self-attention",
    "keys-queries-values"
  ],
  "description": "Implement **single-head self-attention** with learned key, query, and value projections.\n\nGiven an input tensor `x` of shape `(B, T, C)` and a target `head_size`, create three linear projection layers (without bias) that map from `C` to `head_size`. Use these to compute keys, queries, and values from the input. Then compute attention weights from the dot product of queries and keys (with appropriate scaling), apply a causal mask, normalize with softmax, and aggregate the values.\n\nThe output should have shape `(B, T, head_size)`.",
  "starterCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nhead_size = 4\nx = torch.randn(B, T, C)\n\ndef single_head_attention(x, head_size):\n    \"\"\"Compute single-head self-attention with learned projections.\n    \n    Args:\n        x: tensor of shape (B, T, C)\n        head_size: dimension of key/query/value projections\n    Returns:\n        out: tensor of shape (B, T, head_size)\n        wei: attention weights of shape (B, T, T) (after softmax)\n    \"\"\"\n    B, T, C = x.shape\n    # TODO: create key, query, value linear layers (C -> head_size, no bias)\n    # TODO: compute k, q, v by passing x through the layers\n    # TODO: compute attention scores: q @ k.transpose(-2, -1) * head_size**-0.5\n    # TODO: apply causal mask (tril) and softmax\n    # TODO: aggregate values: wei @ v\n    pass\n\nout, wei = single_head_attention(x, head_size)\nprint(\"output shape:\", out.shape)\nprint(\"weights shape:\", wei.shape)\n",
  "solutionCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nhead_size = 4\nx = torch.randn(B, T, C)\n\ndef single_head_attention(x, head_size):\n    \"\"\"Compute single-head self-attention with learned projections.\n    \n    Args:\n        x: tensor of shape (B, T, C)\n        head_size: dimension of key/query/value projections\n    Returns:\n        out: tensor of shape (B, T, head_size)\n        wei: attention weights of shape (B, T, T) (after softmax)\n    \"\"\"\n    B, T, C = x.shape\n    key = nn.Linear(C, head_size, bias=False)\n    query = nn.Linear(C, head_size, bias=False)\n    value = nn.Linear(C, head_size, bias=False)\n    \n    k = key(x)    # (B, T, head_size)\n    q = query(x)  # (B, T, head_size)\n    v = value(x)  # (B, T, head_size)\n    \n    wei = q @ k.transpose(-2, -1) * head_size**-0.5  # (B, T, T)\n    tril = torch.tril(torch.ones(T, T))\n    wei = wei.masked_fill(tril == 0, float('-inf'))\n    wei = F.softmax(wei, dim=-1)  # (B, T, T)\n    out = wei @ v  # (B, T, head_size)\n    return out, wei\n\nout, wei = single_head_attention(x, head_size)\nprint(\"output shape:\", out.shape)\nprint(\"weights shape:\", wei.shape)\n",
  "testCode": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(42)\nB, T, C = 2, 4, 8\nhead_size = 4\nx = torch.randn(B, T, C)\n\nout, wei = single_head_attention(x, head_size)\n\ntry:\n    assert out.shape == (B, T, head_size)\n    print(\"PASS: output shape is (B, T, head_size)\")\nexcept Exception as e:\n    print(f\"FAIL: output shape is (B, T, head_size) — {e}\")\n\ntry:\n    assert wei.shape == (B, T, T)\n    print(\"PASS: attention weight shape is (B, T, T)\")\nexcept Exception as e:\n    print(f\"FAIL: attention weight shape is (B, T, T) — {e}\")\n\ntry:\n    # Check causal masking: upper triangle should be 0 after softmax\n    tril_mask = torch.tril(torch.ones(T, T))\n    upper_vals = wei[:, tril_mask == 0]\n    assert torch.allclose(upper_vals, torch.zeros_like(upper_vals), atol=1e-7)\n    print(\"PASS: upper triangle of attention weights is zero (causal)\")\nexcept Exception as e:\n    print(f\"FAIL: upper triangle of attention weights is zero (causal) — {e}\")\n\ntry:\n    # Check rows sum to 1\n    row_sums = wei.sum(dim=-1)\n    assert torch.allclose(row_sums, torch.ones(B, T), atol=1e-5)\n    print(\"PASS: attention weight rows sum to 1\")\nexcept Exception as e:\n    print(f\"FAIL: attention weight rows sum to 1 — {e}\")\n\ntry:\n    # Different inputs should produce different outputs\n    torch.manual_seed(42)\n    x2 = torch.randn(B, T, C)\n    x_different = torch.randn(B, T, C)\n    out1, _ = single_head_attention(x2, head_size)\n    out2, _ = single_head_attention(x_different, head_size)\n    assert not torch.allclose(out1, out2, atol=1e-3)\n    print(\"PASS: different inputs produce different outputs\")\nexcept Exception as e:\n    print(f\"FAIL: different inputs produce different outputs — {e}\")\n",
  "hints": [
    "Create three nn.Linear(C, head_size, bias=False) layers for key, query, and value. Pass x through each to get k, q, v tensors of shape (B, T, head_size).",
    "Compute raw attention scores: wei = q @ k.transpose(-2, -1) * head_size**-0.5. The scaling by 1/sqrt(head_size) prevents the dot products from becoming too large.",
    "Apply the causal mask: create tril = torch.tril(torch.ones(T, T)), then wei = wei.masked_fill(tril == 0, float('-inf')), then wei = F.softmax(wei, dim=-1). Finally, out = wei @ v."
  ]
}