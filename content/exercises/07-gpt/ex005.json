{
  "id": "07-gpt-ex005",
  "lectureId": "07-gpt",
  "segmentId": "07-gpt-seg04",
  "title": "Train/Validation Split",
  "difficulty": "easy",
  "order": 5,
  "topics": ["data-splitting", "train-val"],
  "runtime": "local",
  "description": "Split the encoded data tensor into a training set and a validation set.\n\n- `train_data` should contain the **first 90%** of the data.\n- `val_data` should contain the **last 10%** of the data.\n\nThis split ensures we can evaluate our model on data it has never seen during training, which is essential for detecting overfitting.",
  "starterCode": "import torch\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\n# TODO: compute the split point n (first 90% of data)\nn = None\n\n# TODO: split data into train_data (first 90%) and val_data (last 10%)\ntrain_data = None\nval_data = None\n",
  "solutionCode": "import torch\n\ntext = \"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\"\n\nchars = sorted(list(set(text)))\nstoi = {ch: i for i, ch in enumerate(chars)}\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\nn = int(0.9 * len(data))\n\ntrain_data = data[:n]\nval_data = data[n:]\n",
  "testCode": "try:\n    expected_n = int(0.9 * len(data))\n    assert n == expected_n, f\"Expected n={expected_n}, got {n}\"\n    print(\"PASS: split point n is correct (90% of data length)\")\nexcept Exception as e:\n    print(f\"FAIL: split point n is correct (90% of data length) — {e}\")\n\ntry:\n    assert isinstance(train_data, torch.Tensor), \"train_data should be a tensor\"\n    assert isinstance(val_data, torch.Tensor), \"val_data should be a tensor\"\n    print(\"PASS: train_data and val_data are tensors\")\nexcept Exception as e:\n    print(f\"FAIL: train_data and val_data are tensors — {e}\")\n\ntry:\n    assert len(train_data) + len(val_data) == len(data), f\"Sizes don't add up: {len(train_data)} + {len(val_data)} != {len(data)}\"\n    print(\"PASS: train and val sizes add up to total data size\")\nexcept Exception as e:\n    print(f\"FAIL: train and val sizes add up to total data size — {e}\")\n\ntry:\n    assert len(train_data) == int(0.9 * len(data)), f\"Expected train size {int(0.9 * len(data))}, got {len(train_data)}\"\n    print(\"PASS: train_data is 90% of the data\")\nexcept Exception as e:\n    print(f\"FAIL: train_data is 90% of the data — {e}\")\n\ntry:\n    assert torch.equal(train_data, data[:n]), \"train_data should be data[:n]\"\n    assert torch.equal(val_data, data[n:]), \"val_data should be data[n:]\"\n    print(\"PASS: train_data and val_data are correct slices of data\")\nexcept Exception as e:\n    print(f\"FAIL: train_data and val_data are correct slices of data — {e}\")\n",
  "hints": [
    "Compute the split index: n = int(0.9 * len(data)).",
    "Use tensor slicing: train_data = data[:n] and val_data = data[n:].",
    "int() truncates toward zero, so for a tensor of length 100, int(0.9 * 100) = 90."
  ]
}