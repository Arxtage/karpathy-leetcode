{
  "id": "01-micrograd-ex021",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg13",
  "title": "Train an MLP on a Tiny Dataset",
  "difficulty": "hard",
  "order": 21,
  "topics": [
    "training",
    "gradient-descent",
    "loss"
  ],
  "description": "Train an MLP to classify a tiny dataset:\n```python\nxs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\nys = [1.0, -1.0, -1.0, 1.0]\n```\n\nImplement a `train()` function that creates an `MLP(3, [4, 4, 1])` and trains it using gradient descent with MSE loss until the loss drops below `0.01`.\n\nReturn `(model, final_loss)`. Don't forget to zero gradients between iterations.",
  "starterCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\ndef train():\n    random.seed(42)\n\n    xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\n    ys = [1.0, -1.0, -1.0, 1.0]\n\n    model = MLP(3, [4, 4, 1])\n    learning_rate = 0.05\n    epochs = 100\n\n    # TODO: Implement the training loop\n    # For each epoch:\n    #   1. Forward pass: compute predictions for all inputs\n    #   2. Compute MSE loss: sum((yout - ygt)**2 for each sample)\n    #   3. Zero all gradients\n    #   4. Backward pass\n    #   5. Update parameters: p.data -= learning_rate * p.grad\n\n    final_loss = 0.0  # Replace with actual loss\n    return model, final_loss",
  "solutionCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\ndef train():\n    random.seed(42)\n\n    xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\n    ys = [1.0, -1.0, -1.0, 1.0]\n\n    model = MLP(3, [4, 4, 1])\n    learning_rate = 0.05\n    epochs = 100\n\n    for epoch in range(epochs):\n        # Forward pass\n        ypred = [model(x) for x in xs]\n\n        # Compute MSE loss\n        loss = sum((yout - ygt) ** 2 for ygt, yout in zip(ys, ypred))\n\n        # Zero gradients\n        for p in model.parameters():\n            p.grad = 0.0\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters\n        for p in model.parameters():\n            p.data -= learning_rate * p.grad\n\n    final_loss = loss.data\n    return model, final_loss",
  "testCode": "try:\n    model, final_loss = train()\n    assert final_loss < 0.05, f\"Expected loss < 0.05, got {final_loss}\"\n    print(f\"PASS: training converged (final loss = {final_loss:.6f})\")\nexcept:\n    print(\"FAIL: training converged\")\n\ntry:\n    model, final_loss = train()\n    xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\n    ys = [1.0, -1.0, -1.0, 1.0]\n    predictions = [model(x) for x in xs]\n    pred_values = [p.data for p in predictions]\n    correct_signs = all(\n        (p > 0 and y > 0) or (p <= 0 and y < 0)\n        for p, y in zip(pred_values, ys)\n    )\n    assert correct_signs, f\"Predictions have wrong signs: {pred_values} vs {ys}\"\n    print(f\"PASS: predictions have correct signs: {[round(p, 3) for p in pred_values]}\")\nexcept:\n    print(\"FAIL: predictions have correct signs\")\n\ntry:\n    model, final_loss = train()\n    params = model.parameters()\n    assert len(params) == 41, f\"Expected 41 parameters, got {len(params)}\"\n    print(\"PASS: model has 41 parameters\")\nexcept:\n    print(\"FAIL: model has 41 parameters\")\n\ntry:\n    model, final_loss = train()\n    xs = [[2.0, 3.0, -1.0], [3.0, -1.0, 0.5], [0.5, 1.0, 1.0], [1.0, 1.0, -1.0]]\n    ys = [1.0, -1.0, -1.0, 1.0]\n    predictions = [model(x).data for x in xs]\n    for pred, target in zip(predictions, ys):\n        assert abs(pred - target) < 0.5, f\"Prediction {pred} too far from target {target}\"\n    print(\"PASS: all predictions within 0.5 of targets\")\nexcept:\n    print(\"FAIL: all predictions within 0.5 of targets\")",
  "hints": [
    "MSE loss: sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))",
    "Zero gradients: for p in model.parameters(): p.grad = 0.0",
    "Update: for p in model.parameters(): p.data -= learning_rate * p.grad",
    "Use random.seed(42) for reproducibility"
  ]
}
