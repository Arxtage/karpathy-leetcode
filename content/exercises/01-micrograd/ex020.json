{
  "id": "01-micrograd-ex020",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg13",
  "title": "Implement an MLP",
  "difficulty": "medium",
  "order": 20,
  "topics": ["neural-networks", "mlp"],
  "description": "Implement an `MLP` (Multi-Layer Perceptron) class:\n- `__init__(self, nin, nouts)`: `nouts` is a list like `[4, 4, 1]`. Create layers: Layer(nin, nouts[0]), Layer(nouts[0], nouts[1]), etc.\n- `__call__(self, x)`: pass input through each layer sequentially\n- `parameters(self)`: return all parameters from all layers\n\nExample: `MLP(3, [4, 4, 1])` creates a network with 3 inputs, two hidden layers of 4 neurons, and 1 output.",
  "starterCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        # TODO: Create layers\n        # Hint: sz = [nin] + nouts gives all layer sizes\n        # Layer i connects sz[i] inputs to sz[i+1] outputs\n        pass\n\n    def __call__(self, x):\n        # TODO: Pass input through each layer sequentially\n        pass\n\n    def parameters(self):\n        # TODO: Return all parameters from all layers\n        pass",
  "solutionCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]\n\n\nclass MLP:\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n    def parameters(self):\n        return [p for layer in self.layers for p in layer.parameters()]",
  "testCode": "import random\nrandom.seed(42)\n\ntry:\n    mlp = MLP(3, [4, 4, 1])\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = mlp(x)\n    assert isinstance(output, Value), f\"Expected single Value output, got {type(output)}\"\n    print(\"PASS: MLP(3, [4, 4, 1]) returns single Value\")\nexcept:\n    print(\"FAIL: MLP(3, [4, 4, 1]) returns single Value\")\n\ntry:\n    mlp = MLP(3, [4, 4, 1])\n    params = mlp.parameters()\n    expected = (3 + 1) * 4 + (4 + 1) * 4 + (4 + 1) * 1\n    assert len(params) == expected, f\"Expected {expected} parameters, got {len(params)}\"\n    print(f\"PASS: MLP(3, [4, 4, 1]) has {expected} parameters\")\nexcept:\n    print(f\"FAIL: MLP(3, [4, 4, 1]) has correct parameter count\")\n\ntry:\n    mlp = MLP(3, [4, 4, 1])\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = mlp(x)\n    assert output.data >= 0, f\"Expected non-negative output (relu), got {output.data}\"\n    print(\"PASS: MLP output is non-negative\")\nexcept:\n    print(\"FAIL: MLP output is non-negative\")\n\ntry:\n    mlp = MLP(3, [4, 4, 1])\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = mlp(x)\n    output.backward()\n    grads_exist = all(hasattr(p, 'grad') for p in mlp.parameters())\n    assert grads_exist, \"All parameters should have gradients\"\n    print(\"PASS: backward works through MLP\")\nexcept:\n    print(\"FAIL: backward works through MLP\")\n\ntry:\n    mlp = MLP(2, [3, 2])\n    params = mlp.parameters()\n    expected = (2 + 1) * 3 + (3 + 1) * 2\n    assert len(params) == expected, f\"Expected {expected} parameters, got {len(params)}\"\n    x = [Value(1.0), Value(2.0)]\n    output = mlp(x)\n    assert isinstance(output, list), f\"Expected list output for nouts[-1]=2, got {type(output)}\"\n    assert len(output) == 2, f\"Expected 2 outputs, got {len(output)}\"\n    print(f\"PASS: MLP(2, [3, 2]) has {expected} params and 2 outputs\")\nexcept:\n    print(\"FAIL: MLP(2, [3, 2]) has correct params and outputs\")",
  "hints": ["sz = [nin] + nouts gives the sizes of all layers", "Layer i connects sz[i] inputs to sz[i+1] outputs"]
}
