{
  "id": "01-micrograd-ex018",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg13",
  "title": "Implement a Neuron",
  "difficulty": "medium",
  "order": 18,
  "topics": [
    "neural-networks",
    "neuron"
  ],
  "description": "Implement a `Neuron` class â€” the basic unit of a neural network.\n\nA neuron takes `nin` inputs, has a weight for each input plus a bias, and computes a single output by combining them linearly and then applying a nonlinearity. Initialize weights randomly in `[-1, 1]`.\n\nInclude a `parameters()` method that returns all trainable values.",
  "starterCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        # TODO: Create nin weights and 1 bias, each a Value with random data in [-1, 1]\n        pass\n\n    def __call__(self, x):\n        # TODO: Compute weighted sum + bias, then apply relu\n        # Hint: use sum(generator, start_value)\n        pass\n\n    def parameters(self):\n        # TODO: Return list of all weights and bias\n        pass",
  "solutionCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]",
  "testCode": "import random\nrandom.seed(42)\n\ntry:\n    n = Neuron(3)\n    params = n.parameters()\n    assert len(params) == 4, f\"Expected 4 parameters (3 weights + 1 bias), got {len(params)}\"\n    print(\"PASS: Neuron(3) has 4 parameters\")\nexcept:\n    print(\"FAIL: Neuron(3) has 4 parameters\")\n\ntry:\n    n = Neuron(3)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = n(x)\n    assert isinstance(output, Value), f\"Expected output to be a Value, got {type(output)}\"\n    assert hasattr(output, 'data'), \"Output should have a data attribute\"\n    print(\"PASS: Neuron output is a Value\")\nexcept:\n    print(\"FAIL: Neuron output is a Value\")\n\ntry:\n    n = Neuron(3)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = n(x)\n    assert output.data >= 0, f\"Expected relu output >= 0, got {output.data}\"\n    print(\"PASS: Neuron output is non-negative (relu applied)\")\nexcept:\n    print(\"FAIL: Neuron output is non-negative (relu applied)\")\n\ntry:\n    n = Neuron(3)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = n(x)\n    output.backward()\n    has_grads = all(hasattr(p, 'grad') for p in n.parameters())\n    assert has_grads, \"All parameters should have gradients after backward\"\n    print(\"PASS: backward works through Neuron\")\nexcept:\n    print(\"FAIL: backward works through Neuron\")\n\ntry:\n    n = Neuron(5)\n    assert len(n.parameters()) == 6, f\"Expected 6 parameters for Neuron(5), got {len(n.parameters())}\"\n    for p in n.parameters():\n        assert isinstance(p, Value), \"Each parameter should be a Value\"\n    print(\"PASS: Neuron(5) has 6 Value parameters\")\nexcept:\n    print(\"FAIL: Neuron(5) has 6 Value parameters\")",
  "hints": [
    "Use random.uniform(-1, 1) for initial weights",
    "sum() needs a start value: sum(generator, self.b)",
    "parameters() returns self.w + [self.b]"
  ]
}
