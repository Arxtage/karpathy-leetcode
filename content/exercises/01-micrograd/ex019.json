{
  "id": "01-micrograd-ex019",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg12",
  "title": "Implement a Layer",
  "difficulty": "medium",
  "order": 19,
  "topics": [
    "neural-networks",
    "layer"
  ],
  "description": "Implement a `Layer` class \u2014 a collection of neurons that all receive the same input.\n\n`Layer(nin, nout)` should create a layer with `nout` neurons, each accepting `nin` inputs. If the layer has a single output neuron, return the `Value` directly rather than a list.\n\nInclude a `parameters()` method.",
  "starterCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        # TODO: Create nout Neurons, each with nin inputs\n        pass\n\n    def __call__(self, x):\n        # TODO: Call each neuron on x\n        # Return single Value if nout==1, else return list\n        pass\n\n    def parameters(self):\n        # TODO: Return all parameters from all neurons\n        pass",
  "solutionCode": "import random\n\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __truediv__(self, other):\n        return self * other**-1\n\n\nclass Neuron:\n    def __init__(self, nin):\n        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n        self.b = Value(0.0)\n\n    def __call__(self, x):\n        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n        return act.relu()\n\n    def parameters(self):\n        return self.w + [self.b]\n\n\nclass Layer:\n    def __init__(self, nin, nout):\n        self.neurons = [Neuron(nin) for _ in range(nout)]\n\n    def __call__(self, x):\n        out = [n(x) for n in self.neurons]\n        return out[0] if len(out) == 1 else out\n\n    def parameters(self):\n        return [p for n in self.neurons for p in n.parameters()]",
  "testCode": "import random\nrandom.seed(42)\n\ntry:\n    l = Layer(3, 4)\n    params = l.parameters()\n    assert len(params) == 16, f\"Expected 16 parameters (4 neurons * (3w + 1b)), got {len(params)}\"\n    print(\"PASS: Layer(3, 4) has 16 parameters\")\nexcept:\n    print(\"FAIL: Layer(3, 4) has 16 parameters\")\n\ntry:\n    l = Layer(3, 4)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = l(x)\n    assert isinstance(output, list), f\"Expected list output for nout=4, got {type(output)}\"\n    assert len(output) == 4, f\"Expected 4 outputs, got {len(output)}\"\n    print(\"PASS: Layer(3, 4) returns list of 4 Values\")\nexcept:\n    print(\"FAIL: Layer(3, 4) returns list of 4 Values\")\n\ntry:\n    l = Layer(3, 1)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = l(x)\n    assert isinstance(output, Value), f\"Expected single Value for nout=1, got {type(output)}\"\n    print(\"PASS: Layer(3, 1) returns single Value\")\nexcept:\n    print(\"FAIL: Layer(3, 1) returns single Value\")\n\ntry:\n    l = Layer(3, 4)\n    x = [Value(1.0), Value(2.0), Value(3.0)]\n    output = l(x)\n    for o in output:\n        assert isinstance(o, Value), f\"Each output should be a Value\"\n        assert o.data >= 0, f\"Each output should be >= 0 (relu), got {o.data}\"\n    print(\"PASS: All Layer outputs are non-negative Values\")\nexcept:\n    print(\"FAIL: All Layer outputs are non-negative Values\")\n\ntry:\n    l = Layer(5, 3)\n    params = l.parameters()\n    assert len(params) == 18, f\"Expected 18 parameters (3 neurons * (5w + 1b)), got {len(params)}\"\n    for p in params:\n        assert isinstance(p, Value), \"Each parameter should be a Value\"\n    print(\"PASS: Layer(5, 3) has 18 Value parameters\")\nexcept:\n    print(\"FAIL: Layer(5, 3) has 18 Value parameters\")",
  "hints": [
    "Each neuron has nin weights + 1 bias",
    "Return single value when nout==1 for convenience"
  ]
}
