{
  "id": "01-micrograd-ex010",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg10",
  "title": "Backward Closure for Addition",
  "difficulty": "medium",
  "order": 10,
  "topics": [
    "backpropagation",
    "closures"
  ],
  "description": "Modify `__add__` so that the resulting `Value` knows how to propagate gradients backward to its inputs.\n\nSet `out._backward` to a function that updates the gradients of the inputs based on `out.grad`. Think about: if `c = a + b`, how does a small change in `a` affect `c`? And remember that a node might be used in multiple operations, so gradients should accumulate.",
  "starterCode": "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._children = set(_children)\n        self._op = _op\n        self._backward = lambda: None\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        # TODO: define a _backward closure that propagates gradients\n        # For addition, the local derivative is 1.0 for both inputs\n        # Remember to use += for gradient accumulation\n        def _backward():\n            pass  # fix this\n\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        return out",
  "solutionCode": "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._children = set(_children)\n        self._op = _op\n        self._backward = lambda: None\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        return out",
  "testCode": "try:\n    a = Value(2.0)\n    b = Value(3.0)\n    c = a + b\n    c.grad = 1.0\n    c._backward()\n    assert a.grad == 1.0\n    print(\"PASS: a.grad is 1.0 after backward\")\nexcept:\n    print(\"FAIL: a.grad is 1.0 after backward\")\n\ntry:\n    a = Value(2.0)\n    b = Value(3.0)\n    c = a + b\n    c.grad = 1.0\n    c._backward()\n    assert b.grad == 1.0\n    print(\"PASS: b.grad is 1.0 after backward\")\nexcept:\n    print(\"FAIL: b.grad is 1.0 after backward\")\n\ntry:\n    a = Value(2.0)\n    b = Value(3.0)\n    c = a + b\n    c.grad = 1.0\n    c._backward()\n    assert c.data == 5.0\n    print(\"PASS: c.data is 5.0\")\nexcept:\n    print(\"FAIL: c.data is 5.0\")\n\ntry:\n    a = Value(2.0)\n    b = Value(3.0)\n    c = a + b\n    d = a + b\n    c.grad = 1.0\n    d.grad = 1.0\n    c._backward()\n    d._backward()\n    assert a.grad == 2.0\n    print(\"PASS: a.grad accumulates to 2.0 when used twice\")\nexcept:\n    print(\"FAIL: a.grad accumulates to 2.0 when used twice\")",
  "hints": [
    "Use += to accumulate gradients, not =",
    "The closure captures self and other from __add__",
    "For addition, the local derivative with respect to both inputs is 1.0"
  ]
}
