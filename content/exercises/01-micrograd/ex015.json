{
  "id": "01-micrograd-ex015",
  "lectureId": "01-micrograd",
  "segmentId": "01-micrograd-seg12",
  "title": "Implement relu for Value",
  "difficulty": "easy",
  "order": 15,
  "topics": [
    "value-class",
    "activation-functions"
  ],
  "description": "Add a `relu()` method to Value that applies the ReLU activation function.\n\nReLU outputs the input if it's positive, and zero otherwise. Include the backward pass — think about what the derivative of ReLU is in each region.",
  "starterCode": "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def relu(self):\n        # TODO: Implement relu\n        # 1. Compute output: max(0, self.data)\n        # 2. Create a new Value with (self,) as children and 'relu' as _op\n        # 3. Define _backward: gradient passes through if out.data > 0, else 0\n        pass",
  "solutionCode": "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n        out = Value(self.data ** other, (self,), f'**{other}')\n        def _backward():\n            self.grad += (other * self.data ** (other - 1)) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for v in reversed(topo):\n            v._backward()\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'relu')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out",
  "testCode": "try:\n    a = Value(3.0)\n    r = a.relu()\n    assert r.data == 3.0, f\"Expected 3.0, got {r.data}\"\n    print(\"PASS: relu of positive value\")\nexcept Exception as e:\n    print(f\"FAIL: relu of positive value — {e}\")\n\ntry:\n    a = Value(-3.0)\n    r = a.relu()\n    assert r.data == 0.0, f\"Expected 0.0, got {r.data}\"\n    print(\"PASS: relu of negative value\")\nexcept Exception as e:\n    print(f\"FAIL: relu of negative value — {e}\")\n\ntry:\n    a = Value(3.0)\n    b = Value(-2.0)\n    c = a * b\n    d = c.relu()\n    d.backward()\n    assert d.data == 0.0, f\"Expected relu(-6) = 0, got {d.data}\"\n    assert a.grad == 0.0, f\"Expected grad 0 (relu blocked), got {a.grad}\"\n    print(\"PASS: backward through relu (negative, blocked)\")\nexcept Exception as e:\n    print(f\"FAIL: backward through relu (negative, blocked) — {e}\")\n\ntry:\n    a = Value(2.0)\n    b = Value(3.0)\n    c = a * b\n    d = c.relu()\n    d.backward()\n    assert d.data == 6.0, f\"Expected relu(6) = 6, got {d.data}\"\n    assert a.grad == 3.0, f\"Expected grad 3.0, got {a.grad}\"\n    assert b.grad == 2.0, f\"Expected grad 2.0, got {b.grad}\"\n    print(\"PASS: backward through relu (positive, passes through)\")\nexcept Exception as e:\n    print(f\"FAIL: backward through relu (positive, passes through) — {e}\")",
  "hints": [
    "relu(x) = max(0, x)",
    "The gradient of relu is 1 if x > 0, else 0"
  ]
}
